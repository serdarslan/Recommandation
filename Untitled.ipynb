{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import IterableDataset, DataLoader, get_worker_info\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class TripletUniformPair(IterableDataset):\n",
    "    def __init__(self, num_item, user_list, pair, shuffle, num_epochs):\n",
    "        self.num_item = num_item\n",
    "        self.user_list = user_list\n",
    "        self.pair = pair\n",
    "        self.shuffle = shuffle\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = get_worker_info()\n",
    "        # Shuffle per epoch\n",
    "        self.example_size = self.num_epochs * len(self.pair)\n",
    "        self.example_index_queue = deque([])\n",
    "        self.seed = 0\n",
    "        if worker_info is not None:\n",
    "            self.start_list_index = worker_info.id\n",
    "            self.num_workers = worker_info.num_workers\n",
    "            self.index = worker_info.id\n",
    "        else:\n",
    "            self.start_list_index = None\n",
    "            self.num_workers = 1\n",
    "            self.index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index >= self.example_size:\n",
    "            raise StopIteration\n",
    "        # If `example_index_queue` is used up, replenish this list.\n",
    "        while len(self.example_index_queue) == 0:\n",
    "            index_list = list(range(len(self.pair)))\n",
    "            if self.shuffle:\n",
    "                random.Random(self.seed).shuffle(index_list)\n",
    "                self.seed += 1\n",
    "            if self.start_list_index is not None:\n",
    "                index_list = index_list[self.start_list_index::self.num_workers]\n",
    "                # Calculate next start index\n",
    "                self.start_list_index = (self.start_list_index + (self.num_workers - (len(self.pair) % self.num_workers))) % self.num_workers\n",
    "            self.example_index_queue.extend(index_list)\n",
    "        result = self._example(self.example_index_queue.popleft())\n",
    "        self.index += self.num_workers\n",
    "        return result\n",
    "\n",
    "    def _example(self, idx):\n",
    "        u = self.pair[idx][0]\n",
    "        i = self.pair[idx][1]\n",
    "        j = np.random.randint(self.num_item)\n",
    "        while j in self.user_list[u]:\n",
    "            j = np.random.randint(self.num_item)\n",
    "        return u, i, j\n",
    "\n",
    "\n",
    "class BPR(nn.Module):\n",
    "    def __init__(self, user_size, item_size, dim, weight_decay):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.empty(user_size, dim))\n",
    "        self.H = nn.Parameter(torch.empty(item_size, dim))\n",
    "        nn.init.xavier_normal_(self.W.data)\n",
    "        nn.init.xavier_normal_(self.H.data)\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, u, i, j):\n",
    "        \"\"\"Return loss value.\n",
    "        \n",
    "        Args:\n",
    "            u(torch.LongTensor): tensor stored user indexes. [batch_size,]\n",
    "            i(torch.LongTensor): tensor stored item indexes which is prefered by user. [batch_size,]\n",
    "            j(torch.LongTensor): tensor stored item indexes which is not prefered by user. [batch_size,]\n",
    "        \n",
    "        Returns:\n",
    "            torch.FloatTensor\n",
    "        \"\"\"\n",
    "        u = self.W[u, :]\n",
    "        i = self.H[i, :]\n",
    "        j = self.H[j, :]\n",
    "        x_ui = torch.mul(u, i).sum(dim=1)\n",
    "        x_uj = torch.mul(u, j).sum(dim=1)\n",
    "        x_uij = x_ui - x_uj\n",
    "        log_prob = F.logsigmoid(x_uij).sum()\n",
    "        regularization = self.weight_decay * (u.norm(dim=1).pow(2).sum() + i.norm(dim=1).pow(2).sum() + j.norm(dim=1).pow(2).sum())\n",
    "        return -log_prob + regularization\n",
    "\n",
    "    def recommend(self, u):\n",
    "        \"\"\"Return recommended item list given users.\n",
    "\n",
    "        Args:\n",
    "            u(torch.LongTensor): tensor stored user indexes. [batch_size,]\n",
    "\n",
    "        Returns:\n",
    "            pred(torch.LongTensor): recommended item list sorted by preference. [batch_size, item_size]\n",
    "        \"\"\"\n",
    "        u = self.W[u, :]\n",
    "        x_ui = torch.mm(u, self.H.t())\n",
    "        pred = torch.argsort(x_ui, dim=1)\n",
    "        return pred\n",
    "\n",
    "\n",
    "def precision_and_recall_k(user_emb, item_emb, train_user_list, test_user_list, klist, batch=512):\n",
    "    \"\"\"Compute precision at k using GPU.\n",
    "\n",
    "    Args:\n",
    "        user_emb (torch.Tensor): embedding for user [user_num, dim]\n",
    "        item_emb (torch.Tensor): embedding for item [item_num, dim]\n",
    "        train_user_list (list(set)):\n",
    "        test_user_list (list(set)):\n",
    "        k (list(int)):\n",
    "    Returns:\n",
    "        (torch.Tensor, torch.Tensor) Precision and recall at k\n",
    "    \"\"\"\n",
    "    # Calculate max k value\n",
    "    max_k = max(klist)\n",
    "\n",
    "    # Compute all pair of training and test record\n",
    "    result = None\n",
    "    for i in range(0, user_emb.shape[0], batch):\n",
    "        # Create already observed mask\n",
    "        mask = user_emb.new_ones([min([batch, user_emb.shape[0]-i]), item_emb.shape[0]])\n",
    "        for j in range(batch):\n",
    "            if i+j >= user_emb.shape[0]:\n",
    "                break\n",
    "            mask[j].scatter_(dim=0, index=torch.tensor(list(train_user_list[i+j])).cuda(), value=torch.tensor(0.0).cuda())\n",
    "        # Calculate prediction value\n",
    "        cur_result = torch.mm(user_emb[i:i+min(batch, user_emb.shape[0]-i), :], item_emb.t())\n",
    "        cur_result = torch.sigmoid(cur_result)\n",
    "        assert not torch.any(torch.isnan(cur_result))\n",
    "        # Make zero for already observed item\n",
    "        cur_result = torch.mul(mask, cur_result)\n",
    "        _, cur_result = torch.topk(cur_result, k=max_k, dim=1)\n",
    "        result = cur_result if result is None else torch.cat((result, cur_result), dim=0)\n",
    "\n",
    "    result = result.cpu()\n",
    "    # Sort indice and get test_pred_topk\n",
    "    precisions, recalls = [], []\n",
    "    for k in klist:\n",
    "        precision, recall = 0, 0\n",
    "        for i in range(user_emb.shape[0]):\n",
    "            test = set(test_user_list[i])\n",
    "            pred = set(result[i, :k].numpy().tolist())\n",
    "            val = len(test & pred)\n",
    "            precision += val / max([min([k, len(test)]), 1])\n",
    "            recall += val / max([len(test), 1])\n",
    "        precisions.append(precision / user_emb.shape[0])\n",
    "        recalls.append(recall / user_emb.shape[0])\n",
    "    return precisions, recalls\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Initialize seed\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    # Load preprocess data\n",
    "    with open(args.data, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "        user_size, item_size = dataset['user_size'], dataset['item_size']\n",
    "        train_user_list, test_user_list = dataset['train_user_list'], dataset['test_user_list']\n",
    "        train_pair = dataset['train_pair']\n",
    "    print('Load complete')\n",
    "\n",
    "    # Create dataset, model, optimizer\n",
    "    dataset = TripletUniformPair(item_size, train_user_list, train_pair, True, args.n_epochs)\n",
    "    loader = DataLoader(dataset, batch_size=args.batch_size, num_workers=16)\n",
    "    model = BPR(user_size, item_size, args.dim, args.weight_decay).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Training\n",
    "    smooth_loss = 0\n",
    "    idx = 0\n",
    "    for u, i, j in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(u, i, j)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar('train/loss', loss, idx)\n",
    "        smooth_loss = smooth_loss*0.99 + loss*0.01\n",
    "        if idx % args.print_every == (args.print_every - 1):\n",
    "            print('loss: %.4f' % smooth_loss)\n",
    "        if idx % args.eval_every == (args.eval_every - 1):\n",
    "            plist, rlist = precision_and_recall_k(model.W.detach(),\n",
    "                                                    model.H.detach(),\n",
    "                                                    train_user_list,\n",
    "                                                    test_user_list,\n",
    "                                                    klist=[1, 5, 10])\n",
    "            print('P@1: %.4f, P@5: %.4f P@10: %.4f, R@1: %.4f, R@5: %.4f, R@10: %.4f' % (plist[0], plist[1], plist[2], rlist[0], rlist[1], rlist[2]))\n",
    "            writer.add_scalars('eval', {'P@1': plist[0],\n",
    "                                                    'P@5': plist[1],\n",
    "                                                    'P@10': plist[2]}, idx)\n",
    "            writer.add_scalars('eval', {'R@1': rlist[0],\n",
    "                                                'R@5': rlist[1],\n",
    "                                                'R@10': rlist[2]}, idx)\n",
    "        if idx % args.save_every == (args.save_every - 1):\n",
    "            dirname = os.path.dirname(os.path.abspath(args.model))\n",
    "            os.makedirs(dirname, exist_ok=True)\n",
    "            torch.save(model.state_dict(), args.model)\n",
    "        idx += 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parse argument\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data',\n",
    "                        type=str,\n",
    "                        default=os.path.join('preprocessed', 'ml-1m.pickle'),\n",
    "                        help=\"File path for data\")\n",
    "    # Seed\n",
    "    parser.add_argument('--seed',\n",
    "                        type=int,\n",
    "                        default=0,\n",
    "                        help=\"Seed (For reproducability)\")\n",
    "    # Model\n",
    "    parser.add_argument('--dim',\n",
    "                        type=int,\n",
    "                        default=4,\n",
    "                        help=\"Dimension for embedding\")\n",
    "    # Optimizer\n",
    "    parser.add_argument('--lr',\n",
    "                        type=float,\n",
    "                        default=1e-3,\n",
    "                        help=\"Learning rate\")\n",
    "    parser.add_argument('--weight_decay',\n",
    "                        type=float,\n",
    "                        default=0.025,\n",
    "                        help=\"Weight decay factor\")\n",
    "    # Training\n",
    "    parser.add_argument('--n_epochs',\n",
    "                        type=int,\n",
    "                        default=500,\n",
    "                        help=\"Number of epoch during training\")\n",
    "    parser.add_argument('--batch_size',\n",
    "                        type=int,\n",
    "                        default=4096,\n",
    "                        help=\"Batch size in one iteration\")\n",
    "    parser.add_argument('--print_every',\n",
    "                        type=int,\n",
    "                        default=20,\n",
    "                        help=\"Period for printing smoothing loss during training\")\n",
    "    parser.add_argument('--eval_every',\n",
    "                        type=int,\n",
    "                        default=1000,\n",
    "                        help=\"Period for evaluating precision and recall during training\")\n",
    "    parser.add_argument('--save_every',\n",
    "                        type=int,\n",
    "                        default=10000,\n",
    "                        help=\"Period for saving model during training\")\n",
    "    parser.add_argument('--model',\n",
    "                        type=str,\n",
    "                        default=os.path.join('output', 'bpr.pt'),\n",
    "                        help=\"File path for model\")\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
